{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from cave.cavefacade import CAVE\n",
    "from ConfigSpace.util import impute_inactive_values\n",
    "from ConfigSpace import CategoricalHyperparameter, UniformFloatHyperparameter, UniformIntegerHyperparameter, OrdinalHyperparameter\n",
    "from cave.utils.helpers import combine_runhistories\n",
    "from cave.utils.helpers import create_random_runhistories, combine_random_local, create_new_rhs  # Julia BA\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "import pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cave = CAVE(folders=[\"../DataForTesting/cplex_regions200/SMAC3/run-1/smac3-output_2019-03-15_09:55:14_185212/run_1/\"],\n",
    "            output_dir=\"./CAVE/Output/June/AutoEncoder/03_06_CAVE_cplex_jupyter_autoencoder/\",\n",
    "            ta_exec_dir=[\"../DataForTesting/cplex_regions200/SMAC3/run-1/\"],\n",
    "            file_format='SMAC3',\n",
    "            # verbose_level='DEBUG'\n",
    "           )\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = cave.scenario\n",
    "from copy import deepcopy\n",
    "configspace = deepcopy(scenario.cs)\n",
    "\n",
    "runhistory = cave.global_original_rh\n",
    "training, transform = create_random_runhistories(runhistory)\n",
    "\n",
    "dicti = configspace._hyperparameters.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.save_json('training.json')\n",
    "import pickle\n",
    "\n",
    "pickle.dump(training, open(\"training.pkl\", 'wb'), protocol=0)\n",
    "pickle.load(open(\"training.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Configspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configspace.get_hyperparameters()                       # List of all Hyperparameter with name, type, choices/interval\n",
    "configspace.get_hyperparameters()[0]                    # Index return hyperparamter at this place\n",
    "configspace.get_hyperparameters()[0].name               # Specification what is needed of this hyperparameter\n",
    "configspace.get_hyperparameter_names()                  # List of the names of all hyperparameter\n",
    "\n",
    "training.get_all_configs_combined()                     # list of all configurations\n",
    "training.get_all_configs_combined()[0]                  # Returns the configuration at the place of index\n",
    "name = configspace.get_hyperparameters()[0].name\n",
    "training.get_all_configs_combined()[0].get(name)        # Get value of the configuration of the defined hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1\n",
    "\n",
    "* standardize continual Data\n",
    "* replace nan with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "for hp in configspace.get_hyperparameters():\n",
    "    if type(hp) is CategoricalHyperparameter:\n",
    "        hp.choices = hp.choices + (-1234, )\n",
    "\n",
    "    \n",
    "values = [OneHotEncoder(categories='auto').fit((np.sort(np.array(hp.choices)).reshape((-1,1))))\n",
    "          if type(hp) is CategoricalHyperparameter \n",
    "          else (StandardScaler().fit(np.array([confi.get(hp.name) for confi in training.get_all_configs_combined()]).reshape(-1, 1))\n",
    "                 if type(hp) in {UniformFloatHyperparameter, UniformIntegerHyperparameter, OrdinalHyperparameter}\n",
    "                 else None)\n",
    "            for hp in configspace.get_hyperparameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(values)):\n",
    "    if type(values[i]) == StandardScaler and type(values[i]) != OneHotEncoder:\n",
    "        pass\n",
    "    elif type(values[i]) == OneHotEncoder and type(values[i]) != StandardScaler:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Fehler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barrier_algorithm ('0', '1', '2', '3', -1234)\n",
      "barrier_crossover ('-1', '0', '1', '2', -1234)\n",
      "barrier_limits_corrections ('-1', '0', '1', '4', '16', '64', -1234)\n",
      "barrier_ordering ('0', '1', '2', '3', -1234)\n",
      "barrier_startalg ('1', '2', '3', '4', -1234)\n",
      "emphasis_memory ('no', -1234)\n",
      "emphasis_mip ('0', '1', '2', '3', '4', -1234)\n",
      "emphasis_numerical ('yes', 'no', -1234)\n",
      "feasopt_mode ('0', '1', '2', '3', '4', '5', -1234)\n",
      "lpmethod ('0', '1', '2', '3', '4', '5', '6', -1234)\n",
      "mip_cuts_cliques ('-1', '0', '1', '2', '3', -1234)\n",
      "mip_cuts_covers ('-1', '0', '1', '2', '3', -1234)\n",
      "mip_cuts_disjunctive ('-1', '0', '1', '2', '3', -1234)\n",
      "mip_cuts_flowcovers ('-1', '0', '1', '2', -1234)\n",
      "mip_cuts_gomory ('-1', '0', '1', '2', -1234)\n",
      "mip_cuts_gubcovers ('-1', '0', '1', '2', -1234)\n",
      "mip_cuts_implied ('-1', '0', '1', '2', -1234)\n",
      "mip_cuts_mcfcut ('-1', '0', '1', '2', -1234)\n",
      "mip_cuts_mircut ('-1', '0', '1', '2', -1234)\n",
      "mip_cuts_pathcut ('-1', '0', '1', '2', -1234)\n",
      "mip_cuts_zerohalfcut ('-1', '0', '1', '2', -1234)\n",
      "mip_limits_cutpasses ('-1', '0', '1', '4', '16', '64', -1234)\n",
      "mip_limits_gomorypass ('0', '1', '4', '16', '64', -1234)\n",
      "mip_ordertype ('0', '1', '2', '3', -1234)\n",
      "mip_strategy_backtrack ('0.9', '0.99', '0.999', '0.9999', '0.99999', '0.999999', -1234)\n",
      "mip_strategy_branch ('-1', '0', '1', -1234)\n",
      "mip_strategy_dive ('0', '1', '2', '3', -1234)\n",
      "mip_strategy_file ('0', '1', -1234)\n",
      "mip_strategy_fpheur ('-1', '0', '1', '2', -1234)\n",
      "mip_strategy_heuristicfreq ('-1', '0', '5', '10', '20', '40', '80', -1234)\n",
      "mip_strategy_lbheur ('yes', 'no', -1234)\n",
      "mip_strategy_nodeselect ('0', '1', '2', '3', -1234)\n",
      "mip_strategy_presolvenode ('-1', '0', '1', '2', -1234)\n",
      "mip_strategy_probe ('-1', '0', '1', '2', '3', -1234)\n",
      "mip_strategy_rinsheur ('-1', '0', '5', '10', '20', '40', '80', -1234)\n",
      "mip_strategy_search ('0', '1', '2', -1234)\n",
      "mip_strategy_startalgorithm ('0', '1', '2', '3', '4', '5', '6', -1234)\n",
      "mip_strategy_subalgorithm ('0', '1', '2', '3', '4', '5', -1234)\n",
      "mip_strategy_variableselect ('-1', '0', '1', '2', '3', '4', -1234)\n",
      "network_netfind ('1', '2', '3', -1234)\n",
      "network_pricing ('0', '1', '2', -1234)\n",
      "preprocessing_aggregator ('-1', '0', '1', '4', '16', '64', -1234)\n",
      "preprocessing_boundstrength ('-1', '0', '1', -1234)\n",
      "preprocessing_coeffreduce ('0', '1', '2', -1234)\n",
      "preprocessing_dependency ('-1', '0', '1', '2', '3', -1234)\n",
      "preprocessing_dual ('-1', '0', '1', -1234)\n",
      "preprocessing_linear ('0', '1', -1234)\n",
      "preprocessing_numpass ('-1', '0', '1', '4', '16', '64', -1234)\n",
      "preprocessing_reduce ('0', '1', '2', '3', -1234)\n",
      "preprocessing_relax ('-1', '0', '1', -1234)\n",
      "preprocessing_repeatpresolve ('-1', '0', '1', '2', '3', -1234)\n",
      "preprocessing_symmetry ('-1', '0', '1', '2', '3', '4', '5', -1234)\n",
      "read_scale ('-1', '0', '1', -1234)\n",
      "sifting_algorithm ('0', '1', '2', '3', '4', -1234)\n",
      "simplex_crash ('-1', '0', '1', -1234)\n",
      "simplex_dgradient ('0', '1', '2', '3', '4', '5', -1234)\n",
      "simplex_limits_perturbation ('0', '1', '4', '16', '64', -1234)\n",
      "simplex_perturbation_switch ('no', 'yes', -1234)\n",
      "simplex_pgradient ('-1', '0', '1', '2', '3', '4', -1234)\n",
      "simplex_pricing ('0', '1', '4', '16', '64', -1234)\n",
      "simplex_refactor ('0', '4', '16', '64', '256', -1234)\n",
      "mip_limits_strongit ('0', '1', '4', '16', '64', -1234)\n",
      "mip_strategy_order ('yes', 'no', -1234)\n"
     ]
    }
   ],
   "source": [
    "config = training.get_all_configs_combined()[0]\n",
    "for hp in configspace.get_hyperparameters():\n",
    "    if type(hp) is CategoricalHyperparameter:\n",
    "        print(hp.name, hp.choices)\n",
    "        \n",
    "# print(config)\n",
    "# print(hp)\n",
    "# OneHotEncoder(categories='auto').fit(np.vstack((np.sort(np.array(hp.choices)).reshape((-1,1)), [[-1]])))\n",
    "#one_hot_encode(training.get_all_configs_combined()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def one_hot_encode(config):\n",
    "    # Code from PhMueller\n",
    "    \n",
    "    # Create array with one hot encoded values\n",
    "    result_vec = np.array([]).reshape((-1, 1))  # .astype(object)\n",
    "\n",
    "    for i, hp in enumerate(configspace.get_hyperparameters()):\n",
    "        \n",
    "        val = np.array(config.get(hp.name)).reshape((-1, 1))\n",
    "        # print(val)\n",
    "        # case if categorical\n",
    "        if type(values[i]) is OneHotEncoder:\n",
    "            if val == [[None]]:\n",
    "                # val = np.array(['-1']).reshape((-1, 1))\n",
    "                val = np.array([['-1234']])\n",
    "            if len(result_vec) == 0:\n",
    "                result_vec = values[i].transform(val).toarray()  # .astype(object)\n",
    "            else:\n",
    "                result_vec = np.hstack((result_vec, \n",
    "                                        values[i].transform(val).toarray()))\n",
    "\n",
    "        # if it is continous\n",
    "        else:\n",
    "            if val == [[None]]:\n",
    "                if len(result_vec) == 0:\n",
    "                    result_vec = np.array([-1234]).reshape((-1, 1))\n",
    "                else:\n",
    "                    result_vec = np.hstack((result_vec, [[-1234]]))\n",
    "            elif len(result_vec) == 0:\n",
    "                result_vec = values[i].transform(val)\n",
    "            else:\n",
    "                result_vec = np.hstack((result_vec, \n",
    "                                        values[i].transform(val)))\n",
    "        \n",
    "    return result_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(values)):\n",
    "    if i == None:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5353\n"
     ]
    }
   ],
   "source": [
    "convert_data = np.array([]).reshape((-1, 1))\n",
    "\n",
    "for confi in range(len(training.config_ids)):\n",
    "    if confi % 500 == 0:\n",
    "        print(confi)\n",
    "    if len(convert_data) == 0:\n",
    "        convert_data = one_hot_encode(training.get_all_configs_combined()[confi])\n",
    "        continue\n",
    "    convert_data = np.vstack((convert_data, one_hot_encode(training.get_all_configs_combined()[confi])))\n",
    "\n",
    "print(len(convert_data))\n",
    "# [one_hot_encode(training.get_all_configs_combined()[confi]) for confi in range(len(training.config_ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_data_transform = np.array([]).reshape((-1, 1))\n",
    "\n",
    "for confi in range(len(transform.config_ids)):\n",
    "    if confi % 10 == 0:\n",
    "        print(confi)\n",
    "    if len(convert_data_transform) == 0:\n",
    "        convert_data_transform = one_hot_encode(transform.get_all_configs_combined()[confi])\n",
    "        continue\n",
    "    convert_data_transform = np.vstack((convert_data_transform, one_hot_encode(transform.get_all_configs_combined()[confi])))\n",
    "\n",
    "print(len(convert_data_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_data.shape[1] == convert_data_transform.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"convert_data.npy\", convert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5353, 349)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(\"convert_data.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_BA(nn.Module):\n",
    "    \"\"\"My own class with softmax and crossentropy to transform tensor back in original strucutre\"\"\"\n",
    "    __constants__ = ['dim']\n",
    "\n",
    "    def __init__(self, num_category, transform_list, confi, dim=None):\n",
    "        super(Softmax_BA, self).__init__()\n",
    "        self.num_cat = num_category\n",
    "        self.transform_list = transform_list\n",
    "        self.configspace = confi\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        indexing = 0\n",
    "        x_ = x.clone()\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        for num in range(len(self.configspace.get_hyperparameters())):\n",
    "            if type(self.transform_list[num]) == OneHotEncoder:\n",
    "                x_[:, indexing:indexing+self.num_cat[num]] = softmax(x[:, indexing:indexing+self.num_cat[num]])\n",
    "                indexing += self.num_cat[num]\n",
    "            else:\n",
    "                indexing += 1\n",
    "        x = x_\n",
    "        return x# Variable(x.data, requires_grad=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\" Our autoencoder class. \"\"\"\n",
    "    \n",
    "    def __init__(self, length, act_f, num_layers):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        if act_f.lower() == 'relu':\n",
    "            self.act_f = torch.nn.ReLU()\n",
    "        else:\n",
    "            self.act_f = torch.nn.Tanh()\n",
    "        \n",
    "        assert num_layers > 1\n",
    "        \n",
    "        self.encoder_layer = nn.ModuleList(\n",
    "            [nn.Linear(int(length/(i+1)), int(length/(i+2))) for i in range(num_layers-1)]\n",
    "        )\n",
    "        self.encoder_layer.extend([nn.Linear(int(length/(num_layers)), 2)])\n",
    "\n",
    "        self.decoder_layer = nn.ModuleList(\n",
    "            [nn.Linear(2, int(length/(num_layers)))]\n",
    "        )\n",
    "        self.decoder_layer.extend(\n",
    "            [nn.Linear(int(length/(i+1)), int(length/(i))) for i in range(num_layers-1, 0, -1)]\n",
    "        )\n",
    "\n",
    "    def encoder(self, x):\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x)\n",
    "            x = self.act_f(x) if i < len(self.encoder_layer) - 1 else x\n",
    "        return x\n",
    "    \n",
    "    def decoder(self, x):\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x)\n",
    "            x = self.act_f(x) if i < len(self.decoder_layer) - 1 else x\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = Softmax_BA(num_cat, values, configspace, dim=1)(x)\n",
    "        return x\n",
    "    \n",
    "    def give_latent_image(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat = []\n",
    "for hp in configspace.get_hyperparameters():\n",
    "    if type(hp) == CategoricalHyperparameter:\n",
    "        num_cat.append(len(hp.choices))\n",
    "    else:\n",
    "        num_cat.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, num_epochs, learning_rate, weight_decay=1e-5, plot_interval=10, verbose=False):\n",
    "    \n",
    "    loss_history = list()\n",
    "    test_loss_history = list()\n",
    "    # criterion = loss_function()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=weight_decay)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Get a new batch of data, 64 key-value pairs in it\n",
    "        ids = np.random.choice(X_train.shape[0], 64, replace=False)\n",
    "        X_train = X_train[ids]\n",
    "        \n",
    "        # Convert to torch tensor, usually you also need to convert to float as in here.\n",
    "        # X_train = torch.tensor(X_train).float()\n",
    "        # X_test = torch.tensor(X_test).float()\n",
    "     \n",
    "        # Forward. Encodes and decodes and gives us the model's prediction.\n",
    "        # model() actually calls 'forward()'\n",
    "        output = model(X_train)\n",
    "        output_test = model(X_test)\n",
    "\n",
    "        # Calculate loss, defined above as mean squared error\n",
    "        loss = loss_function(output, X_train, num_cat)\n",
    "        loss_test = loss_function(output_test, X_test, num_cat)\n",
    "        \n",
    "        \n",
    "        # === The backpropagation\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate new gradients with backpropagation\n",
    "        loss.backward()\n",
    "        # Tune weights accoring to optimizer (it has the learnrate and weight decay as defined above)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # To do output stuff with loss and image, we have to detach() and convert back to numpy.\n",
    "        loss = loss.detach().numpy()\n",
    "        loss_test = loss_test.detach().numpy()\n",
    "        \n",
    "        # Append to loss history\n",
    "        loss_history.append(loss)\n",
    "        test_loss_history.append(loss_test)\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch: ' + str(epoch) + \". Train loss: \" + str(loss.item()) + \" Test loss: \" + str(loss_test.item()))\n",
    "        \n",
    "        if epoch % plot_interval == 0 and epoch != 0:\n",
    "            \n",
    "            print('First 5x5 Dimension of prediction \\n ')\n",
    "            print(X_train[0, 22:31])\n",
    "            print(output[0, 22:31])\n",
    "            print(\"-\"*100)\n",
    "            \n",
    "            \"\"\"low_dim_train = model.give_latent_image(X_train)\n",
    "            low_dim_test = model.give_latent_image(X_test)\n",
    "            low_dim_train = low_dim_train.detach().numpy()\n",
    "            low_dim_test = low_dim_test.detach().numpy()\n",
    "            plt.scatter(low_dim_train[:, 0], low_dim_train[:, 1], s=10.0,label=\"train points\")\n",
    "            plt.scatter(low_dim_test[:, 0], low_dim_test[:, 1], s=10.0,label=\"test points\")           \n",
    "            \n",
    "            plt.legend()\n",
    "            plt.show()\"\"\"\n",
    "    \n",
    "    return loss_history, test_loss_history, model\n",
    "\n",
    "\n",
    "def test(trained_model, X, num_plot):\n",
    "    \"\"\" Test our autoencoder. \"\"\"       \n",
    "    for i in range(num_plot):\n",
    "        \"\"\"index = 0\n",
    "        for cats in num_cat:\n",
    "            if cats == False:\n",
    "                index += 1\n",
    "                continue\n",
    "            plt.bar(np.arange(cats), X[i][index:index+cats], label=\"true\", alpha=0.3)\n",
    "            plt.bar(np.arange(cats), output[i][index:index+cats], label=\"prediction\", alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            index += cats           \n",
    "        print(\"last index true: \" + str(X[i][-1])  + \", prediction: \" + str(output[i][-1]))\"\"\"\n",
    "        \n",
    "        ids = np.random.choice(X.shape[0], 100)\n",
    "        X = X[ids]\n",
    "        \n",
    "        X = torch.tensor(X).float()\n",
    "        output = trained_model(X)\n",
    "        \n",
    "        loss = loss_function(output, X, num_cat)\n",
    "        loss = loss.detach().numpy() \n",
    "        \n",
    "        X = X.detach().numpy()\n",
    "        output = output.detach().numpy()\n",
    "        print(\"Input: \\n %s \\n Output: \\n %s\" % (X[:2, 15:25], output[:2, 15:25]))\n",
    "        print(\"Train loss: \" + str(loss.item()))\n",
    "        print(\"-\" * 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "division = int(len(training.config_ids)* 0.75)\n",
    "\n",
    "ids = np.arange(convert_data.shape[0])\n",
    "np.random.shuffle(ids)\n",
    "train_ids = ids[:division]\n",
    "test_ids = ids[division:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_one_hot(input, target):\n",
    "    _, labels = target.max(dim=1)\n",
    "    return nn.CrossEntropyLoss()(input, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(label, predition, num_category):\n",
    "    indexing = 0\n",
    "    categorical_Loss = 0\n",
    "    mse = nn.MSELoss()\n",
    "    mse_Loss = 0\n",
    "    \n",
    "    for num, hp in enumerate(configspace.get_hyperparameters()):\n",
    "        if type(hp) == CategoricalHyperparameter:\n",
    "            confi_pred = predition[:, indexing:indexing+num_category[num]]\n",
    "            confi_lable = label[:, indexing:indexing+num_category[num]]\n",
    "            categorical_Loss += cross_entropy_one_hot(confi_lable, confi_pred)\n",
    "            indexing += num_category[num]\n",
    "        else:\n",
    "            mse_Loss += mse(label[:, indexing], predition[:, indexing])\n",
    "            indexing += 1\n",
    "    \n",
    "    #print(\"MSE: %s\" % mse_Loss)\n",
    "    #print(\"CE: %s\" % categorical_Loss)\n",
    "    \n",
    "    return mse_Loss + categorical_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New model\n",
    "model = Autoencoder(convert_data.shape[1], \"tanh\", 3)\n",
    "\n",
    "# Train the model and return loss history\n",
    "loss_history, test_loss_history, new_model = train(model, \n",
    "                                        X_train=torch.tensor(convert_data[train_ids]).float(), \n",
    "                                        X_test=torch.tensor(convert_data[test_ids]).float(),\n",
    "                                        num_epochs=1000, \n",
    "                                        learning_rate=1e-5, \n",
    "                                        weight_decay=1e-5, \n",
    "                                        plot_interval=100, \n",
    "                                        verbose=True)\n",
    "\n",
    "# Plot the loss history. Careful: It's the train loss\n",
    "plt.plot(loss_history, label=\"train\")\n",
    "plt.plot(test_loss_history, label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the test loss and plot some example images\n",
    "test(new_model, convert_data_transform, num_plot=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(convert_data).float()\n",
    "Z = torch.tensor(convert_data_transform).float()\n",
    "low_dim_rand = model.give_latent_image(X)\n",
    "low_dim_rand = low_dim_rand.detach().numpy()\n",
    "low_dim_local = model.give_latent_image(Z)\n",
    "low_dim_local = low_dim_local.detach().numpy()\n",
    "\n",
    "plt.scatter(low_dim_rand[:, 0], low_dim_rand[:, 1], s=10.0,label=\"random points\")\n",
    "plt.scatter(low_dim_local[:, 0], low_dim_local[:, 1], s=10.0,label=\"random points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConfigSpace.read_and_write import json\n",
    "\n",
    "with open('./config_space.json', 'w') as f:\n",
    "    f.write(json.write(configspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(convert_data).float()\n",
    "low_dim = model.give_latent_image(X)\n",
    "\n",
    "low_dim = low_dim.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(low_dim[:, 0], low_dim[:, 1],)  # label=\"local points\")\n",
    "\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_costvalue(dists, red_dists):\n",
    "    \"\"\"\n",
    "    Helpfunction to calculate the costvalue to test how big the difference of distance is in the embedding\n",
    "    and original space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dists: np.array, shape(n_samples, n_samples)\n",
    "        Matrix of the distances in the original space.\n",
    "    red_dists: np.array, shape(n_samples, k_dimensions)\n",
    "        Koordinates o\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    costvalue: float\n",
    "        Costvalue of the distances of the two spaces.\n",
    "\n",
    "        costvalues = sum_i sum_j=i+1 (distance_low_space_ij - distance_high_space_ij)\n",
    "    \"\"\"\n",
    "    n_conf = dists.shape[0]\n",
    "    low_dists = euclidean_distances(red_dists)\n",
    "    costvalue = []\n",
    "    mean_actual = []\n",
    "\n",
    "    for i in range(n_conf - 1):\n",
    "        for j in range(i+1, n_conf):\n",
    "            costvalue.append((dists[i][j] - low_dists[i][j])**2)\n",
    "            mean_actual.append(low_dists[i][j])\n",
    "\n",
    "    mean_actual_value = sum(mean_actual) / len(mean_actual)\n",
    "    actual = [(mean_actual_value - dif)**2 for dif in mean_actual]\n",
    "    pred_actual = sum(costvalue)\n",
    "    rse = pred_actual / sum(actual)\n",
    "\n",
    "\n",
    "    costvalue = sum(costvalue) / len(costvalue)\n",
    "    print(\"costvalue\")\n",
    "    print(costvalue)\n",
    "    print(\"rse\")\n",
    "    print(rse)\n",
    "    return costvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "test = torch.randn(2, 3)\n",
    "output = m(test)\n",
    "\n",
    "print(test)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(4, 1, requires_grad=True)\n",
    "target = torch.empty(4, dtype=torch.long).random_(1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(4, dtype=torch.long).random_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = output.detach().numpy()\n",
    "# image = image[0].reshape(image.shape[1])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot-encoder version with -1 for each one-hot dimension &rarr; nan by categorical with 4 choices is [-1, -1, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "def one_hot_encode(config):\n",
    "    # Code from PhMueller\n",
    "    \n",
    "    # Create array with one hot encoded values\n",
    "    result_vec = np.array([]).reshape((-1, 1))  # .astype(object)\n",
    "\n",
    "    for i, name in enumerate(configspace.get_hyperparameter_names()):\n",
    "        \n",
    "        val = np.array(config.get(name)).reshape((-1, 1))\n",
    "        \n",
    "        # Case if this value is not given in the configuration\n",
    "        if val == [[None]]:\n",
    "            # Test, maybe this is not working  \n",
    "            if len(result_vec) == 0 and type(configspace.get_hyperparameter(name)) == CategoricalHyperparameter:\n",
    "                cats = len(configspace.get_hyperparameters()[i].choices)\n",
    "                result_vec = np.array([-1] * cats).reshape((1, len(np.array([-1] * cats))))\n",
    "            elif len(result_vec) == 0 and type(configspace.get_hyperparameter(name)) != CategoricalHyperparameter:\n",
    "                result_vec = np.array([-1]).reshape((-1, 1))\n",
    "            elif len(result_vec) > 0 and type(configspace.get_hyperparameter(name)) == CategoricalHyperparameter:\n",
    "                cats = len(configspace.get_hyperparameters()[i].choices)\n",
    "                result_vec = np.hstack((result_vec, np.array([-1] * cats).reshape((1, len(np.array([-1] * cats))))))\n",
    "            else:\n",
    "                result_vec = np.hstack((result_vec, [[-1]]))\n",
    "        \n",
    "        # case if categorical\n",
    "        elif type(values[i]) is OneHotEncoder:\n",
    "            if len(result_vec) == 0:\n",
    "                result_vec = values[i].transform(val).toarray()  # .astype(object)\n",
    "            else:\n",
    "                result_vec = np.hstack((result_vec, \n",
    "                                        values[i].transform(val).toarray()))\n",
    "\n",
    "        # if it is one\n",
    "        else:\n",
    "            if len(result_vec) == 0:\n",
    "                result_vec = values[i].transform(val)\n",
    "            else:\n",
    "                result_vec = np.hstack((result_vec, \n",
    "                                        values[i].transform(val)))\n",
    "    return result_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abj/anaconda3/envs/visHyp/lib/python3.5/site-packages/numpy/lib/arraysetops.py:518: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n",
      "/home/abj/anaconda3/envs/visHyp/lib/python3.5/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [1] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-76317446550d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/visHyp/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    730\u001b[0m                                        copy=True)\n\u001b[1;32m    731\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/visHyp/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform_new\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;34m\"\"\"New implementation assuming categorical input\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/visHyp/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    120\u001b[0m                     msg = (\"Found unknown categories {0} in column {1}\"\n\u001b[1;32m    121\u001b[0m                            \" during transform\".format(diff, i))\n\u001b[0;32m--> 122\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0;31m# Set the problematic rows to an acceptable value and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories [1] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "oe = OneHotEncoder(categories='auto').fit(np.array([1,2,'-1']).reshape((-1,1)))\n",
    "oe.categories_\n",
    "\n",
    "oe.transform(np.array(1).reshape((-1, 1))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
